
"""
A Classification Model using a LLM and Huggingface Library:
This is a sentiment classification model for good and bad movie reviews.
I use the IMDB dataset from Huggingface for the model training.
This is a runnable code of the model, it is recommended to tune the model with hyperparameters to
improve the results.

Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1bSIRwEhu5mT_0po1X2PUKf38kc5syBLQ

"""

# Install all required packages.
!pip install datasets
!pip install transformers
!pip install accelerate -U
!pip install transformers[torch]

# Import required packages.
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, f1_score
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader
from transformers import AdamW
import torch

# Download the dataset from the Hugging Face library.
dataset = load_dataset("imdb")


print('dataset type', type(dataset))
print('dataset.keys()', dataset.keys())

# Complete the function to split the dataset to train, developement and test sets.
train_data = dataset["train"].train_test_split(test_size=0.1)

train_dataset = train_data["train"]
dev_dataset = train_data["test"]   # This is the developement set
test_dataset = dataset["test"]

print('type(train_dataset): \n', train_dataset)

# Create tokenizer and model instances.
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# Prepare the input, tokenize the movie reviews.
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize_function, batched=True)
dev_dataset = dev_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

print('type(train_dataset): \n', type(train_dataset))

print('len(train_dataset): \n', len(train_dataset))
print('type(train_dataset[0]): \n', type(train_dataset[0]))
print('train_dataset[0].keys()', train_dataset[0].keys())

# Set the dtype to pytorch tensor.
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
dev_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

print('type(train_dataset): \n', type(train_dataset[0]))
print('train_dataset[0].keys()', train_dataset[0].keys())

# The calculations of Deep Neural Networks are computational heavy for standard CPU. Therefore we move the model and the Dataloaders to a GPU for making the calculations.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Set the Hyperparameters
batch_size = 1 # 16       # setting the batch_size to 1 is less computational expensive, but affects the model performance.
learning_rate = 2e-5
epochs = 3

# Complete the function below to create the DataLoaders for the training loop. For demonstration purposes we select just a part of the complete dataset.
train_dataloader = DataLoader(train_dataset.shuffle(seed=42).select(range(10000)), batch_size=batch_size, shuffle=True)
dev_dataloader = DataLoader(dev_dataset.select(range(1000)), batch_size=batch_size, shuffle=False)

# Optimizer
optimizer = AdamW(model.parameters(), lr=learning_rate)

# Training Loop
for epoch in range(epochs):
    # Set the model to 'train' mode.
    model.train()
    total_train_loss=0
    for batch in train_dataloader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_train_loss += loss.item()  # accumulate the loss
        # Backpropagate the loss function:
        loss.backward()
        optimizer.step()

    avg_train_loss = total_train_loss / len(train_dataloader)  # compute average train loss

    # Evaluation on development set
    # Set the model to evaluation mode:
    model.eval()
    total_dev_loss = 0
    with torch.no_grad():
        for batch in dev_dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_dev_loss += outputs.loss.item()
    avg_dev_loss = total_dev_loss / len(dev_dataloader)
    print(f"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_dev_loss:.4f}")

# Evaluate the Performance with Cohen Kappa and F1 metrics.
def get_predictions(model, dataloader):
    model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():  # Disabling gradient calculation is useful for inference. It will reduce memory consumption for computations.
        for batch in dataloader:
            inputs = {key: val.to(device) for key, val in batch.items() if key != "label"}
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1).tolist()
            predictions.extend(preds)
            true_labels.extend(batch["label"])
    return predictions, true_labels

preds, labels = get_predictions(model, DataLoader(test_dataset, batch_size=batch_size))
# Calculate the Cohen's kappa and the f1 scores.
ck = cohen_kappa_score(labels, preds)
f1 = f1_score(labels, preds)
print(f"Cohen Kappa Score: {ck:.3f}")
print(f"F1 Score: {f1:.3f}")

# The predict function, apply the learned model weights to predict what is the sentiment class of a new (unseen) movie review.
def predict(texts, model, tokenizer):
    model.eval()
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=256)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    with torch.no_grad():
        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits
    predicted_class = torch.argmax(logits, dim=1).cpu().numpy()
    return ["positive" if label == 1 else "negative" for label in predicted_class]

# Try it yourself. Replace the text reviews below with your reviews and check what is their sentiment classification.
texts = ["The movie was great!", "I didn't like the film at all."]
predictions = predict(texts, model, tokenizer)
print(predictions)